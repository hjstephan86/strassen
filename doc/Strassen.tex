\documentclass{scrartcl}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage[ngerman]{babel}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{multirow} % Wichtig: Fügen Sie dieses Paket in Ihrer Präambel hinzu!
\usepackage{array} % Oft nützlich in Verbindung mit tabularx

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}

\newtheorem{satz}{Satz}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definiton}[section]
\numberwithin{equation}{section}

\makeatletter
\renewcommand{\ALG@name}{Algorithmus}
\makeatother
\algrenewcommand\algorithmicrequire{\textbf{Eingabe:}}
\algrenewcommand\algorithmicensure{\textbf{Ausgabe:}}

\title{Matrixmultiplikation von \normalfont\scshape{Strassen}}
\author{Stephan Epp\\\texttt{hjstephan86@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\vspace{5em}
\tableofcontents
\newpage

\section{Laufzeit}
Beim Algorithmus von \textsc{Strassen} für die Multiplikation von zwei $n \times n$ Matrizen lautet die Rekurrenzgleichung zur Ermittlung der Laufzeit $T(n)$ des Algorithmus
\begin{align*}
	T(n) = 7 \: T(\tfrac{n}{2}) + c\: n^2.
\end{align*}
Der Algorithmus halbiert in jedem rekursiven Aufruf die beiden $n \times n$ Matrizen zu vier $\tfrac{n}{2} \times \tfrac{n}{2}$ Matrizen. Substituieren wir $n$ durch $\tfrac{n}{2}$, erhalten wir für
\begin{align*}
	T(\tfrac{n}{2}) &= 7 \: T(\tfrac{n}{4}) + c(\tfrac{n}{2})^2 = 7 \: T(\tfrac{n}{4}) + \tfrac{c}{4}n^2.
\end{align*}
Nach dem \textit{ersten} rekursiven Aufruf erhalten wir mit $T(\frac{n}{2})$ eingesetzt in $T(n)$ dann
\begin{align*}
	T(n) &= 7 \: (7 \: T(\tfrac{n}{4}) + \tfrac{c}{4} \: n^2) + c n^2 \\
	&= 7^2\: T(\tfrac{n}{4}) + \tfrac{7}{4} \: cn^2 + c n^2.
\end{align*}
Mit dem \textit{zweiten} rekursiven Aufruf werden die vier $\tfrac{n}{2} \times \tfrac{n}{2}$ Matrizen wieder halbiert zu acht $\tfrac{n}{4} \times \tfrac{n}{4}$ Matrizen. Damit ist
\begin{align*}
	T(\tfrac{n}{4}) &= 7 \: T(\tfrac{n}{8}) + c(\tfrac{n}{4})^2 = 7 \: T(\tfrac{n}{8}) + \tfrac{c}{16}n^2.
\end{align*}
Wird $T(\tfrac{n}{4})$ eingesetzt in $T(n)$ ergibt sich
\begin{align*}
	T(n) &= 7^2\: (7 \: T(\tfrac{n}{8}) + \tfrac{c}{16}n^2) + \tfrac{7}{4}cn^2 + c n^2 \\
	&= 7^3\: T(\tfrac{n}{2^3}) + \tfrac{7^2}{4^2}cn^2 + \tfrac{7}{4}cn^2 + c n^2.
\end{align*}
Betrachten wir nun den $k$-ten rekursiven Aufruf finden wir für 
\begin{align*}
	T(n) = 7^k \: T \: \big(\tfrac{n}{2^k}\big) + cn^2 \: \sum_{i = 0}^{k-1} \bigg(\frac{7}{4}\bigg)^i.
\end{align*}
Zur Vereinfachung belassen wir es bei dem $k$-ten rekursiven Aufruf auch in $T(n)$ bei $k$ und nicht $k + 1$. Kleinere Matrizen als $1 \times 1$ Matrizen gibt es nicht, daher können die $n \times n$ Matrizen nur $k$ mal halbiert werden. Der größte Wert, den $k$ annehmen kann, ist $k = \log_{2}n$. Damit ist
\begin{align*}
	\vphantom{\rule{0pt}{2.5ex}} T(n) &= 7^{\log_{2}n} \: T\Big(\frac{n}{2^{\log_{2}n}}\Big) + cn^2 \: \sum_{i = 0}^{\log_{2}n-1} \left(\tfrac{7}{4}\right)^i \\
	\vphantom{\rule{0pt}{2.5ex}} &= n^{\log_{2}7} \: T(1) + cn^2 \:\frac{\left(\tfrac{7}{4}\right)^{\log_{2}n} - 1}{\tfrac{7}{4} - 1} \\
	\vphantom{\rule{0pt}{2.5ex}} &= O\left(n^{2.8074}\right) + cn^2 \left(\tfrac{7}{4}\right)^{\log_{2}n} \\
	\vphantom{\rule{0pt}{2.5ex}} &= O\left(n^{2.8074}\right) + cn^2 \cdot n^{\log_{2}\tfrac{7}{4}} \\
	\vphantom{\rule{0pt}{2.5ex}} &= O\left(n^{2.8074}\right) + cn^{2.8074} \\
	\vphantom{\rule{0pt}{2.5ex}} &= O\left(n^{2.8074}\right).
\end{align*}

\section{Algorithmus}
Es folgt der Algorithmus \ref{alg:strassen} von \textsc{Strassen}  als Pseudocode. Als Eingabe erhalten wir zwei $n \times n$ Matrizen. Der Einfachheit halber wird angenommen, dass $n$ eine Zweierpotenz ist.
\begin{algorithm}
	\caption{\textsc{Strassen}$(A, B)$}
	\label{alg:strassen}
	\begin{algorithmic}[1]
		\Require $\langle A, B \rangle$, mit $n \times n$ Matrizen $A$, $B$, $n = 2^k$, $k \in \mathbb{N}$
		\Ensure $\langle C \rangle$, mit Produktmatrix $C = AB$
		\If{$n = 1$} \textbf{return} $C = AB$
		\EndIf
		%\Comment{Matrizen in $n/2 \times n/2$ Blöcke unterteilen}
		\State $A = \begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}$
		\State $B = \begin{pmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{pmatrix}$
		%\Comment{Berechne die 7 Produkte rekursiv}
		\State $P_1 = \textsc{Strassen}(A_{11} + A_{22}, B_{11} + B_{22})$
		\State $P_2 = \textsc{Strassen}(A_{21} + A_{22}, B_{11})$
		\State $P_3 = \textsc{Strassen}(A_{11}, B_{12} - B_{22})$
		\State $P_4 = \textsc{Strassen}(A_{22}, B_{21} - B_{11})$
		\State $P_5 = \textsc{Strassen}(A_{11} + A_{12}, B_{22})$
		\State $P_6 = \textsc{Strassen}(A_{21} - A_{11}, B_{11} + B_{12})$
		\State $P_7 = \textsc{Strassen}(A_{12} - A_{22}, B_{21} + B_{22})$
		% \Comment{Berechne die Blöcke von $C$}
		\State $C_{11} = P_1 + P_4 - P_5 + P_7$
		\State $C_{12} = P_3 + P_5$
		\State $C_{21} = P_2 + P_4$
		\State $C_{22} = P_1 - P_2 + P_3 + P_6$
		\State \textbf{return} $C = \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix}$
	\end{algorithmic}
\end{algorithm}
Zu Beginn prüfen wir, ob die Größe der Matrizen bereits den Wert $1$ hat. Haben die Matrizen den Wert $1$, geben wir das Produkt $A B$ zurück. In Zeile 2 und 3 werden die Matrizen $A$ und $B$ so definiert, dass in den Zeilen 4 bis 10 die 5 Matrixmultiplikationen jeweils durchgeführt werden. In den Zeilen 11 bis 14 werden 4 Matrizen $C_{ij}$ durch Addition und Subtraktion der Matrizen $A_{ij}$ berechnet und in Zeile 15 Matrix $C$ als Ergebnis zurückgegeben.

Als Idee zum Beweis der Korrektheit betrachten wir das Produkt $A \cdot B$ der zwei Matrizen $A$ und $B$ mit
\begin{align*}
	A = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \quad \text{und} \quad B = \begin{pmatrix} e & f \\ g & h \end{pmatrix}.
\end{align*}
Zur Vereinfachung beinhalten die Matrizen nur skalare Werte. Das Ergebnis $C = A \cdot B$ ist
\begin{align*}
	C = \begin{pmatrix} ae + bg & af + bh \\ ce + dg & cf + dh \end{pmatrix}.
\end{align*}
Der Algorithmus von \textsc{Strassen} berechnet $P_r$ mit
\[
\begin{array}{ll}
	P_1 = \textsc{Strassen}(a + d, e + h) & = ae + ah + de + dh, \\
	P_2 = \textsc{Strassen}(c + d, e)     & = ce + de, \\
	P_3 = \textsc{Strassen}(a, f - h)     & = af - ah, \\
	P_4 = \textsc{Strassen}(d, g - e)     & = dg - de, \\
	P_5 = \textsc{Strassen}(a + b, h)     & = ah + bh, \\
	P_6 = \textsc{Strassen}(c - a, e + f) & = ce + cf - ae - af, \\
	P_7 = \textsc{Strassen}(b - d, g + h) & = bg + bh - dg - dh.
\end{array}
\]
Dann werden $C_{ij}$ berechnet mit
\[
\begin{array}{ll}
	C_{11} = P_1 + P_4 - P_5 + P_7 & = ae + bg, \\
	C_{12} = P_3 + P_5 & = af + bh, \\
	C_{21} = P_2 + P_4 & = ce + dg, \\
	C_{22} = P_1 - P_2 + P_3 + P_6 & = cf + dh,
\end{array}
\]
wobei z.B. $ C_{11} = (ae + ah + de + dh) + (dg - de) - (ah + bh) + (bg + bh - dg - dh) = ae + bg$. Für einen formalen Beweis der Korrektheit verzichten wir auf die vollständige Induktion über $n \in \mathbb{N}$ der $n \times n$ Matrizen.

\section{Experimentelle Ergebnisse}
Um die theoretische Laufzeitanalyse von \textsc{Strassen}s Algorithmus zu überprüfen, wurde eine Python-Implementierung des Algorithmus erstellt und deren Performance mit der einer Standard-Matrixmultiplikation verglichen. Die Experimente wurden für Matrizen unterschiedlicher Größe ($n$) durchgeführt, wobei $n$ von 4 bis 256 in Schritten von 4 variiert wurde. Für jede Matrixgröße wurden 5 Messungen (Trials) durchgeführt und die durchschnittliche Laufzeit ermittelt.
\begin{table}[h!]
	\centering
	\caption{Vergleich der Matrixmultiplikationen}
	\label{tab:strassen-results}
	\begin{tabular}{c|m{3.3cm}|m{3.3cm}}
		\hline
		\multirow{2}{*}{$n$} & \multicolumn{2}{c}{Durchschnittliche Laufzeit (s)} \\ % Multicolumn für die Überschrift
		\cline{2-3} % Linie unter der Multicolumn
		& Standard & \textsc{Strassen} \\ % Unterüberschrift
		\hline
		\hline
		4 & 0.000027 & 0.000045 \\ 
		8 & 0.000101 & 0.000142 \\ 
		12 & 0.000312 & 0.000698 \\ 
		16 & 0.000708 & 0.000808 \\ 
		20 & 0.001350 & 0.004538 \\  
		24 & 0.002304 & 0.004827 \\ 
		28 & 0.003644 & 0.005236 \\ 
		32 & 0.005369 & 0.005674 \\ 
		36 & 0.007291 & 0.035827 \\ 
		40 & 0.010007 & 0.036722 \\ 
		44 & 0.013256 & 0.037815 \\ 
		48 & 0.017039 & 0.038888 \\ 
		52 & 0.021834 & 0.034057 \\ 
		\textcolor{orange}{56} & \textcolor{orange}{0.013749} & \textcolor{orange}{0.020460} \\ 
		60 & 0.016938 & 0.020990 \\ 
		64 & 0.020041 & 0.021484 \\ 
		\vdots & \vdots & \vdots \\ 
		% 68 & 0.024157 & 0.133145 \\ % Auskommentierte Datenzeile
		% 72 & 0.028486 & 0.133083 \\ % Auskommentierte Datenzeile
		% 76 & 0.033549 & 0.143920 \\ % Auskommentierte Datenzeile
		% 80 & 0.039826 & 0.141013 \\ % Auskommentierte Datenzeile
		% 84 & 0.045262 & 0.141792 \\ % Auskommentierte Datenzeile
		% 88 & 0.051738 & 0.152629 \\ % Auskommentierte Datenzeile
		% 92 & 0.059471 & 0.146050 \\ % Auskommentierte Datenzeile
		% 96 & 0.067689 & 0.147936 \\ % Auskommentierte Datenzeile
		% 100 & 0.076471 & 0.162647 \\ % Auskommentierte Datenzeile
		% 104 & 0.087148 & 0.154874 \\ % Auskommentierte Datenzeile
		% 108 & 0.095150 & 0.155554 \\ % Auskommentierte Datenzeile
		% 112 & 0.107501 & 0.158186 \\ % Auskommentierte Datenzeile
		% 116 & 0.118892 & 0.171497 \\ % Auskommentierte Datenzeile
		% 120 & 0.131730 & 0.165475 \\ % Auskommentierte Datenzeile
		% 124 & 0.148736 & 0.166968 \\ % Auskommentierte Datenzeile
		% 128 & 0.161912 & 0.179074 \\ % Auskommentierte Datenzeile
		% 132 & 0.175445 & 0.999888 \\ % Auskommentierte Datenzeile
		% 136 & 0.191708 & 1.045382 \\ % Auskommentierte Datenzeile
		% 140 & 0.217163 & 1.082742 \\ % Auskommentierte Datenzeile
		% 144 & 0.233093 & 1.084697 \\ % Auskommentierte Datenzeile
		% 148 & 0.256793 & 1.078266 \\ % Auskommentierte Datenzeile
		% 152 & 0.272154 & 1.076065 \\ % Auskommentierte Datenzeile
		% 156 & 0.310149 & 1.115063 \\ % Auskommentierte Datenzeile
		% 160 & 0.324504 & 1.094453 \\ % Auskommentierte Datenzeile
		% 164 & 0.343517 & 1.101307 \\ % Auskommentierte Datenzeile
		% 168 & 0.363632 & 1.107035 \\ % Auskommentierte Datenzeile
		% 172 & 0.391884 & 1.117648 \\ % Auskommentierte Datenzeile
		% 176 & 0.418512 & 1.160525 \\ % Auskommentierte Datenzeile
		% 180 & 0.448960 & 1.159266 \\ % Auskommentierte Datenzeile
		% 184 & 0.471133 & 1.147314 \\ % Auskommentierte Datenzeile
		% 188 & 0.513541 & 1.139447 \\ % Auskommentierte Datenzeile
		% 192 & 0.542235 & 1.151873 \\ % Auskommentierte Datenzeile
		% 196 & 0.571728 & 1.158231 \\ % Auskommentierte Datenzeile
		% 200 & 0.608602 & 1.184203 \\ % Auskommentierte Datenzeile
		% 204 & 0.661712 & 1.198355 \\ % Auskommentierte Datenzeile
		% 208 & 0.683460 & 1.221877 \\ % Auskommentierte Datenzeile
		% 212 & 0.751752 & 1.258453 \\ % Auskommentierte Datenzeile
		% 216 & 0.793948 & 1.281702 \\ % Auskommentierte Datenzeile
		% 220 & 0.810436 & 1.224457 \\ % Auskommentierte Datenzeile
		% 224 & 0.864762 & 1.219905 \\ % Auskommentierte Datenzeile
		% 228 & 0.909023 & 1.232622 \\ % Auskommentierte Datenzeile
		% 232 & 0.962290 & 1.250792 \\ % Auskommentierte Datenzeile
		% 236 & 1.000335 & 1.258274 \\ % Auskommentierte Datenzeile
		240 & 1.055068 & 1.256394 \\ 
		244 & 1.105579 & 1.272495 \\ 
		248 & 1.158183 & 1.285980 \\ 
		252 & 1.221187 & 1.295652 \\ 
		\textcolor{red}{256} & \textcolor{red}{1.298067} & \textcolor{red}{1.286444} \\
		\hline
	\end{tabular}
\end{table}
Ein interner Schwellwert (threshold) von 32 wurde für \textsc{Strassen}s Algorithmus festgelegt, was bedeutet, dass für Matrizen, deren Größe kleiner oder gleich 32 ist, auf die Standard-Matrixmultiplikation umgeschaltet wird, um den zusätzliche Aufwand der Rekursion zu reduzieren. Die Systemauslastung vor Beginn der Experimente betrug 0,0\% CPU-Auslastung bei 3.38 GB verwendetem RAM von insgesamt 7.01 GB. Nach Abschluss der Experimente betrug die CPU-Auslastung 3.8\% und der verwendete RAM 3.43 GB, was auf eine moderate Systemauslastung während der Messungen hinweist. Die Messergebnisse sind in Tabelle \ref{tab:strassen-results} zusammengefasst.

Aus der Tabelle lässt sich ablesen, dass für kleine Matrizen (z.B. $n \le 64$) die Standard-Matrixmultiplikation tendenziell schneller war oder eine vergleichbare Performance wie \textsc{Strassen}s Algorithmus aufwies. Dies ist auf den zusätzlichen Aufwand der Rekursion und der zusätzlichen Matrixadditionen/-subtraktionen bei \textsc{Strassen}s Algorithmus zurück-zuführen. Insbesondere ist der Sprung in der Laufzeit des Algorithmus bei $n=36$ (nach dem eingestellten Schwellenwert von $n=32$) auffällig, was den Wechsel von der optimierten Basis-Multiplikation zur rekursiven Struktur widerspiegelt. Erst ab $n=256$ übertrifft \textsc{Strassen} die Standardmultiplikation leicht. Die tatsächliche Laufzeit wird jedoch stark vom gewählten Schwellwert und der Effizienz der Implementierung der Basisfälle beeinflusst. Die Abbildung \ref{fig:time-comparison} zeigt die Verläufe der benötigten Rechenzeit beider Matrixmultiplikationen. Für $n = 56$ waren sowohl die Standard als auch \textsc{Strassen}s Multiplikation schneller als für vorherige $n < 56$.

\begin{figure}[h]
	\centering
	\caption{Vergleich der Laufzeit der Matrixmultiplikationen}
	\includegraphics[width=1.0\textwidth]{results.pdf}
	\label{fig:time-comparison}
\end{figure}

\textsc{Strassen} verwendet 7 Matrixmultiplikationen für zwei Matrizen der Größe $n \times n$. Lässt sich die Anzahl der benötigten Matrixmultiplikationen reduzieren?

\section{Optimierung}
Um die Anzahl der benötigten Matrixmultiplikationen von 7 auf 5 zu reduzieren, wird die Diagonale $e$ und $h$ der Matrix $B$ betrachtet. Dadurch wird für $n$ eine Laufzeit im Exponenten von $2.3219$ statt $2.807$ erreicht. Betrachtet man $P_2 = (c + d) \cdot e = ce + de$ und $P_5 = (a + b) \cdot h = ah + bh$, dann fällt auf, dass $P_2$ durch $P_6$ und $P_1$ ermittelt werden kann,
\begin{align*}
	ce &= P_6 - cf + ae + af, \\
	de &= P_1 - ae - ah - dh,
\end{align*}
und $P_5$ durch $P_1$ und $P_7$
\begin{align*}
	ah &= P_1 - ae - de - dh, \\
	bh &= P_7 - bg + dg + dh.
\end{align*}
Dazu müssen $P_1$, $P_6$ und $P_7$ berechnet werden bevor $P_2$ und $P_5$ ohne Multiplikation bestimmt werden können:
\begin{align*}
	P_2 &= ce + de = (P_6 - cf + ae + af) + (P_1 - ae - ah - dh) = P_6 + P_1 - cf + af - ah + af, \\
	P_5 &= ah + bh = (P_1 - ae - de - dh) + (P_7 - bg + dg + dh) = P_1 + P_7 - ae - de - bg + dg.
\end{align*}
Da nur 5 Matrixmultiplikationen verwendet werden, ergibt sich für die Laufzeitanalyse 
\begin{align*}
	\vphantom{\rule{0pt}{2.5ex}} T(n) &= 5^{\log_{2}n} \: T\Big(\frac{n}{2^{\log_{2}n}}\Big) + cn^2 \sum_{i = 0}^{\log_{2}n-1} \left(\tfrac{5}{4}\right)^i
	\vphantom{\rule{0pt}{2.5ex}} = n^{\log_{2}5} \: T(1) + cn^2 \frac{\left(\tfrac{5}{4}\right)^{\log_{2}n} - 1}{\tfrac{5}{4} - 1}
	\vphantom{\rule{0pt}{2.5ex}} = O\left(n^{2.3219}\right).
\end{align*}
Dazu wird der Algorithmus von \textsc{Strassen} zu \textsc{Strassen-25} so geändert, dass $P_2$ und $P_5$ in Zeile 9 und 10 ermittelt werden. Damit $P_2$ und $P_5$ ermittelt werden können, müssen die zuvor ermittelten Produkte $P_r$, $r \in \{1, 6, 7\}$ verwendet werden. Das bedeutet für 
\begin{align*}
	P_2 &= P_6 + P_1 - A_{21}B_{12} + A_{11}B_{12} - A_{11}B_{22} + A_{11}B_{12}, \\
	P_5 &= P_1 + P_7 - A_{11}B_{11} - A_{22}B_{11} - A_{21}B_{21} + A_{22}B_{21}.
\end{align*}
Dabei ist $P_2$ \textit{abhängig} von $P_6$ und $P_7$ und $P_5$ ist \textit{abhängig} von $P_1$ und $P_7$. Damit $P_2$ und $P_5$ ohne Matrixmultiplikation ermittelt werden können, müssen die Produkte $A_{ij}B_{ij}$ im vorherigen Rekursionsschritt jeweils gespeichert werden.
\begin{algorithm}
	\caption{\textsc{Strassen-25}$(A, B)$}
	\label{alg:strassen25}
	\begin{algorithmic}[1]
		\Require $\langle A, B \rangle$, mit $n \times n$ Matrizen $A$, $B$, $n = 2^k$, $k \in \mathbb{N}$
		\Ensure $\langle C, A_{ij}, B_{ij} \rangle$, mit Produktmatrix $C = AB$ und $A_{ij}$, $B_{ij}$
		\If{$n = 1$} \textbf{return} $C = AB$
		\EndIf
		%\Comment{Matrizen in $n/2 \times n/2$ Blöcke unterteilen}
		\State $A = \begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}$
		\State $B = \begin{pmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{pmatrix}$
		%\Comment{Berechne die 7 Produkte rekursiv}
		\State $P_1 = \textsc{Strassen-25}(A_{11} + A_{22}, B_{11} + B_{22})$
		\State $P_3 = \textsc{Strassen-25}(A_{11}, B_{12} - B_{22})$
		\State $P_4 = \textsc{Strassen-25}(A_{22}, B_{21} - B_{11})$
		\State $P_6 = \textsc{Strassen-25}(A_{21} - A_{11}, B_{11} + B_{12})$
		\State $P_7 = \textsc{Strassen-25}(A_{12} - A_{22}, B_{21} + B_{22})$
		\State $P_2 = P_6 + P_1 - A_{21}B_{12} + A_{11}B_{12} - A_{11}B_{22} + A_{11}B_{12}$ 
		\State $P_5 = P_1 + P_7 - A_{11}B_{11} - A_{22}B_{11} - A_{21}B_{21} + A_{22}B_{21}$
		% \Comment{Berechne die Blöcke von $C$}
		\State $C_{11} = P_1 + P_4 - P_5 + P_7$
		\State $C_{12} = P_3 + P_5$
		\State $C_{21} = P_2 + P_4$
		\State $C_{22} = P_1 - P_2 + P_3 + P_6$
		\State \textbf{return} $C = \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix}$
	\end{algorithmic}
\end{algorithm}
% Bei der Implementierung von \textsc{Strassen-25}$(A, B)$ müssen die Produkte $A_{ij}B_{ij}$ in jedem Rekursionsschritt für die gesamte Rekursion in einer Datenstruktur gespeichert werden. Dabei muss auf die Datenstruktur mit einem eindeutigen Schlüssel $(k, i_{A}, j_{A}, i_{B}, j_{B})$ in $O(1)$ zugegriffen werden, wobei $n = 2^k$, $k \in \mathbb{N}$.
Damit in jedem Rekursionsschritt die Produkte $A_{ij}B_{ij}$ effizient gespeichert werden können, muss auf eine Datenstruktur mit einem eindeutigen Schlüssel $(k, i_{A}, j_{A}, i_{B}, j_{B})$ in $O(1)$ zugegriffen werden, $n = 2^k$, $k \in \mathbb{N}$.
\begin{satz}
	Es gibt keinen Teile-und-Herrsche Algorithmus zur Multiplikation von $n \times n$ Matrizen, der durch Nutzen von Abhängigkeiten weniger als 8 Matrixmultiplikationen benötigt und damit auch weniger als 5 Matrixmultiplikationen benötigt. 
\end{satz}
\begin{proof}
	Angenommen, es gibt einen Teile-und-Herrsche Algorithmus zur Multiplikation von $n \times n$ Matrizen, der mit weniger als 5 Matrixmultiplikationen auskommt, dann muss dieser Algorithmus auch Abhängigkeiten $P_r = (P_s, P_t, A_{ij}B_{ij})$ verwenden, mit denen $P_r$ ermittelt werden kann, $r < 5$. Da $dim(A_{ij}) = dim(B_{ij}) = \frac{n}{2}$, ist es nicht möglich $C = AB$ zu berechnen. Das gesamte Bild der \textit{projezierten} Produktmatrix $C$ kann nur projeziert werden, wenn mindestens mehr als die Hälfte beider Matrizen $A_{ij}$ und $B_{ij}$ zum Projezieren verfügbar ist.
\end{proof}
\begin{satz}
	Das optimale Teile-und-Herrsche Prinzip ist die effizienteste Methode, um Probleme zu lösen, wenn die Probleminstanz bei jedem rekursiven Aufruf halbiert wird. 
\end{satz}
\begin{proof}
	In diesen drei Kriterien ist der Beweis zu führen: (1) Rekursionstiefe, (2) Anzahl der Teilprobleme, (3) Größe eines Teilproblems. Dabei wird schnell klar, dass es alles in der Halbierung zusammengefasst optimal ist. Denn die Halbierung ist nichts anderes als eine Verdoppelung im Nenner.
	
	Zu (1): Wichtig in der Analyse der Laufzeit eines Algorithmus oder einer Lösungsvorschrift, bestehend aus einfachen Operationen und dem Aufruf von Funktionen, ist das Betrachten der gerufenen Funktionen. Weniger wichtig sind einfache Operationen und Zuweisungen. Daher ist für die Laufzeitanalyse die Rekursionstiefe von wichtigster Bedeutung (sieht man z.B. beim Algorithmus von \textsc{Strassen} oder beim Algorithmus \textsc{Merge-Sort}).
	
	Die Rekursionstiefe ist logarithmisch. Kleiner kann die Tiefe für die gesamte Lösung nicht sein, da das Problem mit der 2 im Nenner logarithmisch geteilt wird. Die Funktion, die am stärksten wächst und dabei immer noch eine Umkehrfunktion hat, ist die Exponentialfunktion. Daher ist ihre Umkehrfunktion am schnellsten in der Reduzierung der Rekursionstiefe für die Laufzeitanalyse für Teile-und-Herrsche Algorithmen. Eine Erhöhung des Nenners führt zwangsläufig zu einer Veränderung der anderen beiden Kriterien.
	
	Zu (2), (3): Für die Anzahl der Teilprobleme und für die Größe eines Teilproblems gilt: Angenommen, der Nenner wird vergrößert. Dann wird die ursprüngliche Probleminstanz aber nicht in ihrem leichtesten Punkt geteilt. Denn ein Problem ist nur da am leichtesten zu teilen, wo sein Schwerpunkt liegt. Es gibt für jede Probleminstanz nur einen Schwerpunkt. Wenn der Nenner ganzzahlig verkleinert wird, wird nicht mehr geteilt. Es muss ganzzahlig verkleinert werden, da z.B. ein einzelnes Array-Element nicht teilbar ist. Die Zahl $e$ scheidet damit in der Wahl der Exponentialfunktion aus. Daher muss die Exponentialfunktion 2 als Basis haben.
\end{proof}
Auffällig ist, dass nur das Problem, bei dem zwei Instanzen durch einen Operator verbunden sind, mit dem optimalen Teile-und-Herrsche Prinzip zu lösen ist. Das sind Probleme mit Operatoren wie z.B., $+$, $-$, $<$. Im Prinzip werden diese Probleme an ihrem Schwerpunkt gelöst, wo sie trotz ihrer Schwere am leichtesten zu tragen sind, wie bei einer Wippe mit zwei gleichen Gewichten jeweils am äußersten Ende. Die Wippe ist dann exakt horizontal ausgerichtet im Gleichgewicht.
\begin{lemma}
	Schneller als mit $T(n) = O(n^{2.3219})$ können zwei $n \times n$ Matrizen nicht multipliziert werden.
\end{lemma}
\begin{proof}
Der Beweis folgt daraus, dass \textsc{Strassen-25}$(A, B)$ zwei $n \times n$ Matrizen nach dem optimalen Teile-und-Herrsche Prinzip multipliziert.
\end{proof}

\begin{satz}
Wenn es bei einer Lösung erforderlich ist, jedes Element der Eingabe während der Abarbeitung der Lösungsvorschrift einmal zu betrachten, dann gibt es keine bessere Laufzeit als die logarithmische.
\end{satz}
\begin{proof}
	Wenn Laufzeit so aufgefasst wird, dass in jedem Schritt ein Register besucht wird, dann kann die Anzahl der unterschiedlich besuchten Register nicht weniger als logarithmisch sein. Die Sonnenblumenkerne in einer Sonnenblume sind logarithmisch angeordnet und damit optimal untergebracht mit minimal wenig Platz. Sie sind im goldenen Winkel angebracht. Würden sie besser angebracht werden können, dann wäre der goldene Winkel kein goldener Winkel.
\end{proof}

\begin{lemma}
	Das optimale Teile-und-Herrsche Prinzip ist optimal unter allen Lösungs-prinzipien, wo es während der Abarbeitung der Lösungsvorschrift erforderlich ist, jedes Element der Eingabe während der Lösung einmal zu betrachten.
\end{lemma}
\begin{proof}
	Der Beweis folgt daraus, dass Algorithmen, die nach dem optimalen Teile-und-Herrsche Prinzip Lösungsvorschriften abarbeiten, logarithmische Laufzeit haben müssen. 
\end{proof}

\subsection{Komplexitätsklassen}
Algorithmen folgen einem Lösungsprinzip nach dem sie ein Problem lösen. Es gibt zum Beispiel das optimale Teile-und-Herrsche Prinzip oder das Prinzip der dynamischen Programmierung. Es kann sich lohnen, die Klasse $\mathbf{P}$ in Komplexitätsklassen einzuteilen, bei der eine Klasse durch ein Lösungsprinzip und ihre charakteristische Laufzeit beschrieben wird. Zum Beispiel kann man sich die Klasse vorstellen, in der alle Probleme enthalten sind, die nach dem optimalen Teile-und-Herrsche Prinzip zu lösen sind und nicht besser. Da die charakteristische Laufzeit dieser Klasse die logarithmische ist, könnte sie den Namen $\mathbf{L}$ haben. Man kann sich auch die Klasse $\mathbf{P_2}$ vorstellen, in der alle Probleme enthalten sind, die nach dem Prinzip der dynamischen Programmierung zu lösen sind und für deren Lösung quadratische Laufzeit benötigt wird.

\begin{definition}
	Die Klasse $\mathbf{P}$ enthält die Menge aller Probleme, für die es jeweils einen Algorithmus gibt, der das Problem in polynomieller Laufzeit löst.	
\end{definition}

\begin{definition}
	Die Klasse $\mathbf{NP}$ enthält die Menge aller Probleme, von denen bekannt ist, dass es für sie jeweils keinen Algorithmus gibt, der das Problem noch in polynomieller Laufzeit lösen kann.
\end{definition}
Bisher hat man vermutet, dass zwischen $\mathbf{P}$ und $\mathbf{NP}$ eine Beziehung derart existiert, dass $\mathbf{P} \subseteq \mathbf{NP}$. Diese Vermutung ist aber nicht richtig.
\begin{satz}
	Die Klassen $\mathbf{P}$ und $\mathbf{NP}$ sind gleich, d.h., $\mathbf{P} = \mathbf{NP}$.
\end{satz}
\begin{proof}
	Betrachte das Problem $\mathbf{S}$, die eindeutige Schneeflocke. Gegeben ist eine Schneeflocke mit 5 gleichlangen Armen. Wie wird diese Schneeflocke eindeutig unter allen bereits vorhandenen Schneeflocken? Finde einen Algorithmus, der in polynomieller Zeit immer eine neue Schneeflocke erzeugt. Beweis durch Induktion:\newline
	IB: Sei $S$ die Menge aller eindeutigen Schneeflocken, o.B.d.A.\newline
	IA: $|S| = 0$ trivial, $|S| = 1$ trivial\newline
	IS: $|S| \rightarrow |S| + 1$, $|S| = n, n \in \mathbb{N}$\newline
	Ziel: Mache aus 8 Knoten wieder 6 Knoten mit gleicher Kantenlänge in polynomieller Zeit. Berechne dazu die Restklasse 6. Jeder Knoten berechnet eine Funktion $f(b_1, \ldots, b_n) = \{0, \ldots, 9\}$, $b_i \in \{0, 1\}$. Bei Hinzufügen zwei neuer Knoten erhöht sich die Wertemenge des vorhandenen Graphen um maximal $18\equiv 0 \mod 6$. \newline
	Die Vermutung liegt nahe, das Problem $\mathbf{S}$ als NP-vollständig einzuschätzen. Da dieses Problem aber in polynomieller Zeit lösbar ist, ist es echt in $\mathbf{P}$. 
\end{proof}
Da $\mathbf{P} = \mathbf{NP}$, lohnt es sich gerade auch die Probleme genauer zu betrachten, von denen man vorher vermutete, dass es für sie keinen Algorithmus gibt, der das Problem in polynomieller Laufzeit lösen kann.

Vielleicht kann man in Zukunft ein Problem der Klasse $\mathbf{P_2}$ auch effizienter lösen und damit der Klasse $\mathbf{L}$ zuordnen. Man kann sich die Klasse $\mathbf{P_3}$ vorstellen, in der alle Probleme enthalten sind, die nach dem Prinzip der dynamischen Programmierung zu lösen sind und für deren Lösung kubische Laufzeit benötigt wird (die Lösungstabelle hat eine dritte Dimension). Auch hier kann es sein, dass für ein Problem dieser Klasse eine effizientere Lösung gefunden wird und sie damit einer leichteren Komplexitätsklasse zugeordnet werden kann. Da ein Problem auch immer weniger effizient, d.h., langsamer, gelöst werden kann, ist es nicht so, dass z.B. ein Problem der Klasse $\mathbf{L}$ auch der Klasse $\mathbf{P_2}$ zuzuordnen ist, da man Probleme in der Praxis nicht langsamer löst. Alle Klassen sind in $\mathbf{P}$ enthalten aber untereinander nicht:
\begin{align*}
	\mathbf{L} \subset \mathbf{P}, \quad \mathbf{P_2} \subset \mathbf{P}, \quad \mathbf{P_3} \subset \mathbf{P}, \quad \mathbf{L} \not \subset \mathbf{P_2}, \quad \mathbf{P_2} \not \subset \mathbf{P_3}, \quad \mathbf{L} \not \subset \mathbf{P_3}.
\end{align*}
Beispiele für Probleme, die in $\mathbf{L}$ enthalten sind, sind die $n \times n$ Matrixmultiplikation und das Sortieren von ganzen Zahlen. Beide Probleme lassen sich mit dem optimalen Teile-und-Herrsche Prinzip lösen.

\subsection{Experimentelle Ergebnisse}
Bei der Analyse der zu messenden Laufzeit von Algorithmus \textsc{Strassen-25} ist zu erwarten, dass dieser mit nur 5 Matrixmultiplikationen deutlich weniger Rechenzeit benötigt als der Algorithmus von \textsc{Strassen} mit 7 Matrixmultiplikationen.

Die Tabelle \ref{tab:operations-comparison} zeigt die theoretische Anzahl benötigter Operationen für die Matrixmultiplikation in Abhängigkeit von $n$. Die Operationen sind elementare arithmetische Operationen wie Multiplikationen und Additionen/Subtraktionen von Skalaren. Die Anzahl der Operationen in der Tabelle sind asymptotisch und ignorieren konstante Faktoren, um die Skalierungseffekte zu betonen.
\begin{table}[h]
	\centering
	\caption{Vergleich der theoretischen Anzahl an Operationen}
	\label{tab:operations-comparison}
	\renewcommand{\arraystretch}{1.5} % Zeilenhöhe erhöhen
	\begin{tabular}{m{1.5cm}|m{3cm}|m{3cm}|m{3cm}|m{2cm}}
		\hline
		$n$ & Standard & \textsc{Strassen} & \textsc{Strassen-25} & Matrixgröße ($n \times n$) \\
		\hline\hline
		10 & $10^3 = 1.000$ & $10^{2.8074} \approx 642$ & $10^{2.3219} \approx 209$ & $100$ \\
		\hline
		100 & $100^3 = 10^6$ & $100^{2.8074} \approx 6.4 \times 10^5$ & $100^{2.3219} \approx 2.09 \times 10^4$ & $10^4$ \\
		\hline
		1.000 & $1.000^3 = 10^9$ & $1.000^{2.8074} \approx 6.4 \times 10^8$ & $1.000^{2.3219} \approx 2.09 \times 10^7$ & $10^6$ \\
		\hline
		10.000 & $10.000^3 = 10^{12}$ & $10.000^{2.8074} \approx 6.4 \times 10^{11}$ & $10.000^{2.3219} \approx 2.09 \times 10^{9}$ & $10^8$ \\
		\hline
		100.000 & $100.000^3 = 10^{15}$ & $100.000^{2.8074} \approx 6.4 \times 10^{14}$ & $100.000^{2.3219} \approx 2.09 \times 10^{11}$ & $10^{10}$ \\
		\hline
		1.000.000 & $1.000.000^3 = 10^{18}$ & $1.000.000^{2.8074} \approx 6.4 \times 10^{17}$ & $1.000.000^{2.3219} \approx 2.09 \times 10^{13}$ & $10^{12}$ \\
		\hline
		$10^7$ & $10^{21}$ & $6.4 \times 10^{20}$ & $2.09 \times 10^{15}$ & $10^{14}$ \\
		\hline
		$10^8$ & $10^{24}$ & $6.4 \times 10^{23}$ & $2.09 \times 10^{17}$ & $10^{16}$ \\
		\hline
	\end{tabular}
\end{table}
Die Wahl des optimalen Algorithmus zur Matrixmultiplikation hängt stark von der Größe der Matrizen und dem spezifischen Anwendungsbereich ab. Interessant ist, dass schon für $n = 100$ nur $10^{4}$ Operationen benötigt werden von \textsc{Strassen-25} und dagegen $10^{5}$ Operationen von \textsc{Strassen}. Für $n \geq 10.000$ unterscheiden sich \textsc{Strassen-25} und \textsc{Strassen} um 2 Zehnerpotenzen und für $n \geq 10^8$ unterscheiden sie sich um 6 Zehnerpotenzen. Praktische Anwendungsbereiche für die $n \times n$ Matrixmultiplikation sind die folgenden:

\begin{enumerate}
	\item \textbf{Kleine Matrizen} ($n < 200$): Für kleinere Matrizen, wie sie in der Computergrafik oder bei einfachen linearen Gleichungssystemen vorkommen, ist der Standardalgorithmus mit $O(n^3)$ aufgrund seines geringen zusätzlichen Aufwands für die Rekursion und optimaler Cache-Nutzung die erste Wahl.
		
	\item \textbf{Mittlere bis große Matrizen} ($n \approx 200 - 10.000$): Im wissenschaftlichen Rechnen, bei Optimierungsproblemen oder im maschinellen Lernen dominieren weiterhin optimierte Implementierungen des Standardalgorithmus.
	
	\item \textbf{Sehr große Matrizen} ($n \ge 10.000$): Bei großen Matrizen, wie sie in Big Data Analysen oder beim Training umfangreicher Deep-Learning-Modelle auftreten, wird der Bedarf an Alternativen zu optimierten Standardalgorithmen deutlich. Damit gewinnt der Algorithmus von \textsc{Strassen} an Attraktivität und Relevanz für die Bewältigung rechenintensiver Multiplikationen.
\end{enumerate}

\section{Signalübertragung}
Für die Signalübertragung ist es wichtig, das Signal vom Sender zum Empfänger sicher zu übertragen. Welche kleinste Einheit eignet sich dabei als die optimale? Bisher betrachtet wurde: Es gibt die 2 zum Teilen beim optimalen Teile-und-Herrsche Prinzip. Es gibt die Exponentialfunktion zur Basis 2. Es gibt die logarithmische Laufzeit. Darüber hinaus gibt es das Binärsystem, bei dem die kleinste Einheit durch 2 Zustände repräsentiert wird.
\begin{satz}
	Das Binärsystem ist optimal für die Signalübertragung, bei der elektronische Signale genutzt werden: für an (1) und für aus (0).
\end{satz}
\begin{proof}
	Mit einer 2 Bitfolge können 4 Werte übertragen werden, mit einer 3 Bitfolge können 8 Werte übertragen werden. Die Anzahl der übertragbaren Werte ist exponentiell zur Basis 2 in der Anzahl der Bits in der zu übertragenden Bitfolge. Das heißt, die Anzahl an Bits ist logarithmisch zur Wertigkeit, die übertragbar ist. Warum ist 3 oder 4 als Basis nicht besser? Es ist deshalb nicht besser, da man vom Querschnitt der Leitung aus betrachtet so einen maximalen Abstand zwischen (0) und (1) hat zur sicheren, lesbaren Signalübertragung und Störungen (wie durch Rauschen) so maximal entgegengewirkt wird. In der Mitte des Leitungsquerschnitts ist der Schwerpunkt der Leitung. Nur bei 2 Zuständen (0) und (1) mit gleichem Abstand zum Schwerpunkt der Leitung ist der Abstand aller Signale voneinander (maximal) und zum Schwerpunkt der Leitung (minimal) optimal.	
\end{proof}
Das heißt, der Leitungsquerschnitt wird somit so klein wie möglich gehalten und damit ist so wenig Leitungsmaterial wie nötig erforderlich. Es ist aus wirtschaftlicher und fairer Sicht nicht vertretbar, städtische Haushalte mit einem anderen als dem Binärsystem für die Signalübertragung zu versorgen. Zugleich verfügen ländliche Gebiete oft nicht über dieselbe Internetbandbreite wie Städte. Die Debatte um die Nutzung von Quantenphysik für die Signalübertragung erübrigt sich somit unter wirtschaftlichen und gerechten Gesichtspunkten. Solange die Quantenphysik nicht für die Signalübertragung eingesetzt wird, ist auch die Verarbeitung von Signalen bzw. Daten mittels Quantenphysik nicht sinnvoll.

Die \textit{Shannon-Hartley-Gleichung} beschreibt die maximale theoretische Datenrate, d.h., \textit{Kanalkapazität C}, die über einen Kommunikationskanal fehlerfrei übertragen werden kann, unter Berücksichtigung von Bandbreite und Rauschen. Sie lautet:
\begin{align*}
	\text{C} = \text{B} \cdot \log_2{\bigg(1+\frac{\text{S}}{\text{N}}\bigg)}, \quad \text{dabei ist}
\end{align*}
\begin{enumerate}
	\item[C] Kanalkapazität in Bit pro Sekunde (bit/s)
	\item[B] Bandbreite des Kanals in Hertz (Hz)
	\item[S] Durchschnittliche Signalleistung über den Kanal
	\item[N] Durchschnittliche Rauschleistung über den Kanal
	\item[S/N] Signal-Rausch-Verhältnis (SNR)
\end{enumerate}
Das Signal-Rausch-Verhältnis (SNR) ist dabei der Quotient aus der durchschnittlichen Leistung zur Signalübertragung und der durchschnittlichen Rauschleistung im Kanal:
\begin{align*}
	\text{S}/\text{N} = 10 \cdot \log_{10} \bigg(\frac{\text{P}_{\text{Sign}}}{\text{P}_{\text{Nois}}}\bigg).
\end{align*}
Beträgt die Signalleistung zum Beispiel $\text{P}_{\text{Sign}} = 100\text{mW}$ und die Rauschleistung $\text{P}_{\text{Sign}} = 1\text{mW}$, dann ist 
\begin{align*}
	\text{S}/\text{N} = 10 \cdot \log_{10} \bigg(\frac{100\text{mW}}{1{\text{mW}}}\bigg) = 20.
\end{align*}
Das Signal-Rausch-Verhältnis wird nicht in dB angegeben, da es nur ein Verhältnis beschreibt, in dem zwei Leistungen, die Signalleistung und die Rauschleistung, zueinander stehen. Tabelle \ref{tab:snr} zeigt typische SNR-Wertebereiche. Bei einem SNR, das größer als 40 ist, ist eine optimale Signalübertragung möglich.
\begin{table}[h!]
	\centering
	\caption{Typische SNR-Wertebereiche und deren Bedeutung für die Signalqualität}
	\label{tab:snr}
	\begin{tabular}{p{2cm}|p{2.5cm}|p{5cm}|p{4cm}}
		\hline
		SNR & Qualitäts-einstufung & Beschreibung / Typische Auswirkungen & Anwendungsbeispiele (Tendenz) \\
		\hline\hline
		$< 0$            & Unbrauchbar & Rauschleistung überwiegt Signalleistung deutlich. Keine Kommunikation möglich oder extrem gestört. & Sehr schlechte/keine Mobilfunkverbindung, gestörtes WLAN am Rand. \\
		\hline
		$0 - 5$          & Sehr schlecht & Signal kaum von Rauschen zu unterscheiden. Häufige Abbrüche, extrem langsame/unzuverlässige Datenübertragung. & Schwaches WLAN, sehr schlechter Radioempfang. \\
		\hline
		$5 - 10$         & Schlecht & Verbindung instabil, Störungen. Datenraten stark eingeschränkt, viele Wiederholungen nötig. & Grundlegende Internetnutzung (E-Mails), VoIP mit Aussetzern. \\
		\hline
		$10 - 15$        & Akzeptabel & Brauchbare Verbindung. Leichte Qualitätseinbußen, gelegentliche Verzögerungen. Für Grundaufgaben ausreichend. & Web-Browse, normale Videoanrufe, nicht-kritisches Streaming. \\
		\hline
		$15 - 25$        & Gut & Stabile, zuverlässige Verbindung mit guter Qualität. Für die meisten Anwendungen ausreichend. & HD-Streaming, Online-Gaming (Casual), allgemeine Netzwerkaktivitäten. \\
		\hline
		$25 - 40$        & Sehr Gut & Exzellente Bedingungen, kaum Störeinflüsse. Hohe Datenraten und sehr stabile Verbindungen. & 4K-Streaming, anspruchsvolles Online-Gaming, professionelle Audio/Video, Unternehmensnetzwerke. \\
		\hline
		$> 40$           & Optimal & Außergewöhnlich klare, störungsfreie Signale. Ideal für kritische Anwendungen, höchste Signalintegrität. & Hochpräzise Messsysteme, Laborequipment, Spezialanwendungen. \\
		\hline
	\end{tabular}
\end{table}
Hochpräzise Messsysteme und Spezialanwendungen nutzen dieses Verhältnis. Bei einem SNR, das zwischen 25 und 40 liegt, ist die Qualität für die Signalübertragung sehr gut und ermöglicht z.B. 4K-Streaming. Bei einem SNR zwischen 10 und 15 ist die Qualität akzeptabel für normale Videoanrufe oder nicht-kritisches Streaming.

\section{Der BMM-Witnesses-Algorithmus: Wege finden in Graphen}
Mit dem Algorithmus von \textsc{Strassen-25} lassen sich $n \times n$ Matrizen effizient multiplizieren. Matrizen sind nicht nur für arithmetische Berechnungen nützlich; sie spielen auch eine zentrale Rolle in der Graphentheorie. Insbesondere können damit Fragen zur Erreichbarkeit in Graphen beantworten. Man kann sich vorstellen, man hat eine Karte (einen Graphen) und möchten wissen, ob man von Punkt A nach Punkt B gelangen kann. Der BMM-Witnesses-Algorithmus (Boolean Matrix Multiplication with Witnesses) hilft dabei, solche Verbindungen zu finden und sogar zu identifizieren, über welchen Zwischenpunkt eine solche Verbindung verläuft.

Im Gegensatz zur üblichen Matrixmultiplikation, bei der wir Produkte addieren, verwenden wir hier logische Operationen. Anstelle der Multiplikation ($\times$) wird die logische AND-Operation ($\land$) und anstelle der Addition (+) die logische OR-Operation ($\lor$) genutzt. Die Elemente der Matrizen sind dabei binäre Werte, also entweder 0 (aus) oder 1 (an).

Seien $A$ und $B$ zwei $n \times n$ Boolesche Matrizen. Das Produkt $C = A \cdot B$ ist dann eine $n \times n$ Matrix, wobei jedes Element $C_{ij}$ wie folgt berechnet wird:
\begin{equation}
	\label{formula:bmm}
	C_{ij} = \bigvee_{k=1}^{n} (A_{ik} \land B_{kj}).
\end{equation}
Ein Wert $C_{ij} = 1$ bedeutet, dass es einen Pfad von Knoten $i$ zu Knoten $j$ der Länge 2 gibt. Das liegt daran, dass $A_{ik} = 1$ bedeutet, es gibt eine Kante von $i$ nach $k$, und $B_{kj} = 1$ bedeutet, es gibt eine Kante von $k$ nach $j$. Wenn beides wahr ist ($A_{ik} \land B_{kj} = 1$), dann existiert ein Pfad von $i \to k \to j$. Die OR-Operation ($\bigvee$) sorgt dafür, dass $C_{ij} = 1$ ist, wenn es mindestens einen solchen Zwischenknoten $k$ gibt.

\subsection{Die Rolle des "Witness" (Zeugen)}
Der BMM-Witnesses-Algorithmus geht über die reine Feststellung der Erreichbarkeit hinaus. Er liefert zusätzlich einen \textit{Zeugen} für jede gefundene Verbindung. Ein Zeuge für einen Pfad $i \to k \to j$ ist der Zwischenknoten $k$. Der Algorithmus erstellt eine dritte Matrix, die \textit{Witness-Matrix} $W$, in der $W_{ij}$ den Index des ersten gefundenen Zwischenknotens $k$ speichert, für den $A_{ik} \land B_{kj} = 1$ ist. Wenn kein solcher Pfad existiert ($C_{ij} = 0$), dann ist $W_{ij} = 0$.

Dies ist besonders nützlich, um nicht nur zu wissen, ob ein Pfad existiert, sondern auch wie man dorthin gelangt. Im Kontext von Graphen kann man so die direkten Nachfolger auf einem 2-Schritt-Pfad identifizieren. Der Algorithmus zur Berechnung des Booleschen Matrixprodukts $C$ und der Witness-Matrix $W$ ist relativ einfach und ähnelt der Standard-Matrixmultiplikation, nur mit angepassten Operationen.

\begin{algorithm}
	\caption{BMM-Witnesses$(A, B)$}
	\label{alg:bmm_witnesses}
	\begin{algorithmic}[1]
		\Require $\langle A, B \rangle$, mit $n \times m$ Matrix $A$ und $m \times p$ Matrix $B$. Die Elemente sind 0 oder 1.
		\Ensure $\langle C, W \rangle$, mit Produktmatrix $C$ ($n \times p$) und Witness-Matrix $W$ ($n \times p$)
		
		\State Initialisiere $C$ als $n \times p$ Matrix mit Nullen
		\State Initialisiere $W$ als $n \times p$ Matrix mit Nullen
		
		\For{$i = 1$ \textbf{to} $n$}
		\For{$j = 1$ \textbf{to} $p$}
		\For{$k = 1$ \textbf{to} $m$}
		\If{$A_{ik} = 1 \land B_{kj} = 1$}
		\State $C_{ij} = 1$
		\If{$W_{ij} = 0$} \Comment{Speichere den ersten gefundenen Zeugen}
		\State $W_{ij} = k+1$ \Comment{Oft $k+1$ um 0 als \textit{kein Zeuge} zu nutzen}
		\EndIf
		\EndIf
		\EndFor
		\EndFor
		\EndFor
		\State \textbf{return} $\langle C, W \rangle$
	\end{algorithmic}
\end{algorithm}

Der Algorithmus durchläuft alle möglichen Tripel $(i, j, k)$. Wenn eine Kante von $i$ nach $k$ und eine Kante von $k$ nach $j$ existiert ($A_{ik}=1$ und $B_{kj}=1$), dann wird $C_{ij}$ auf 1 gesetzt. Gleichzeitig wird der Index $k$ (hier als $k+1$ gespeichert, um 0 als Default-Wert für "kein Zeuge" zu reservieren) in $W_{ij}$ eingetragen, sofern noch kein Zeuge für diese Position gefunden wurde. Die Laufzeit dieses Algorithmus ist $O(n \cdot m \cdot p)$, also $O(n^3)$ für quadratische Matrizen, ähnlich der Standard-Matrixmultiplikation.

Der BMM-Witnesses-Algorithmus findet breite Anwendung in Bereichen wie:
\begin{itemize}
	\item[-] Graphentheorie: Bestimmung der Transitivität (Existenz von Pfaden) in Graphen und Auffinden von Gliedern in diesen Pfaden.
	\item[-] Soziale Netzwerkanalyse: Identifizierung von Personen, die indirekt miteinander verbunden sind, und über welche gemeinsamen Kontakte diese Verbindung zustande kommt.
	\item[-] Routenplanung: In einfachen Fällen kann er helfen, Zwischenstationen auf Routen zu finden.
	\item[-] Datenbanken: Abfragen von indirekten Beziehungen zwischen Entitäten.
\end{itemize}

Während \textsc{Strassens} Algorithmus die Laufzeit für die arithmetische Matrixmultiplikation verbessert, ist der BMM-Witnesses-Algorithmus ein fundamentales Werkzeug, das die Mächtigkeit von Matrizen für logische Schlussfolgerungen und die Analyse von Beziehungen in komplexen Netzwerken demonstriert.

\subsection{Zeugen für Boolesche Matrixmultiplikation}

Seien $A$ und $B$ zwei $n \times n$ Boolesche Matrizen, deren Einträge entweder 0 oder 1 sind. Das Boolesche Produkt $P$ von $A$ und $B$ ist eine Matrix, deren Einträge $p_{ij}$ wie folgt definiert sind:
$$p_{ij}=\bigvee_{k=1}^{n}(a_{ik}\wedge b_{kj}).$$
Hierbei repräsentieren $\bigvee$ und $\wedge$ die Booleschen Operatoren ODER und UND. Dies bedeutet, dass $p_{ij}=1$ genau dann ist, wenn es mindestens ein $k$ gibt, für das sowohl $a_{ik}=1$ als auch $b_{kj}=1$ gilt.

\subsubsection{Berechnung des Booleschen Produkts}
Das Boolesche Produkt $P=AB$ kann effizient berechnet werden. Man kann die 0/1-Einträge als ganze Zahlen behandeln und ein herkömmliches Matrixprodukt $M$ (mit ganzen Zahlen) berechnen. Dann ist $p_{ij}=1$ genau dann, wenn der entsprechende Eintrag $m_{ij}$ in der Matrix $M$ größer als 0 ist. Die Zeitkomplexität für diese Berechnung liegt bei $O(M(n))$, wobei $M(n)$ die Zeit ist, die zur Multiplikation zweier $n \times n$ Matrizen benötigt wird, was wiederum $o(n^3)$ ist, wenn man Strassens Algorithmus oder schnellere Methoden verwendet.

\subsubsection{Die Notwendigkeit von Zeugen}
In einigen Anwendungen ist es nicht ausreichend zu wissen, dass $p_{ij}=1$. Man möchte zusätzlich einen Index $k$ finden, für den $a_{ik}=1$ und $b_{kj}=1$ gilt. Dieser Index $k$ wird als \textit{Zeuge} bezeichnet. Eine Zeugenmatrix $W$ für $P=AB$ ist eine Matrix, deren Einträge $w_{ij}$ wie folgt definiert sind:
$$w_{ij}=\begin{cases} 0 & \text{falls } p_{ij}=0 \\ k \text{ mit } a_{ik}=b_{kj}=1 & \text{falls } p_{ij}=1 \end{cases}$$
Die Schwierigkeit besteht darin, eine solche Zeugenmatrix in subkubischer Zeit zu berechnen, da ein naiver Ansatz, der jedes $k$ für jedes Paar $(i, j)$ überprüft, zu einer Laufzeit von $O(n^3)$ führen würde.

\subsubsection{Ansatz bei eindeutigem Zeugen}
Angenommen, für ein Paar $(i, j)$ mit $p_{ij}=1$ gibt es einen eindeutigen Zeugen $k_{ij}$. In diesem speziellen Fall könnte man eine modifizierte Matrix $\hat{A}$ definieren, indem man $\hat{a}_{ik} = k \cdot a_{ik}$ setzt. Das $(i, j)$-Element des Produkts $\hat{A}B$ wäre dann $k_{ij}$, also der korrekte Zeuge. Wenn der Zeuge jedoch nicht eindeutig ist, würde der Eintrag im Produkt $\hat{A}B$ "Müll" enthalten, und man könnte den Index nicht direkt bestimmen. Ein einfacher Ansatz, $\hat{a}_{ik}=2^k a_{ik}$ zu verwenden, um alle Zeugen zu identifizieren, würde zu Problemen mit der Größe der Zahlen führen, da dann $n$-Bit-Zahlen verwendet würden und die Annahme von $O(1)$ Zeit für Ganzzahloperationen nicht mehr gültig wäre.

\subsubsection{Randomisierter Ansatz für allgemeine Zeugen}
Eine allgemeine Lösung für das Finden von Zeugen kann mithilfe von Randomisierung erreicht werden. Sei $w_{ij}$ die Anzahl der Zeugen für $p_{ij}=1$. Wir definieren eine modifizierte Matrix $\hat{A}$ mit $\hat{a}_{ik} = r_k \cdot a_{ik}$, wobei $r_k$ eine Zufallsvariable ist. Genauer gesagt, $r_k=1$ mit einer Wahrscheinlichkeit $\pi$ und $r_k=0$ mit Wahrscheinlichkeit $1-\pi$. Die Wahrscheinlichkeit $\pi$ wird so gewählt, dass $1/(2w_{ij}) \le \pi < 1/w_{ij}$ ist.

\subsubsection{Wahrscheinlichkeit eines Zeugen}
Der Kern des randomisierten Ansatzes ist die Behauptung, dass die Wahrscheinlichkeit, dass die Summe $\sum_{k=1}^{n} \hat{a}_{ik} b_{kj}$ ein Zeuge für $p_{ij}=1$ ist, mindestens $1/(2e)$ beträgt, wobei $e$ die Eulersche Zahl ist. Dies kann wie folgt bewiesen werden: Angenommen, es gibt $w$ "weiße Kugeln" (Zeugen) und $n-w$ "schwarze Kugeln" (keine Zeugen). Wenn jede der $n$ Kugeln unabhängig voneinander mit Wahrscheinlichkeit $\pi$ ausgewählt wird, ist die Wahrscheinlichkeit $\rho$, dass genau eine weiße Kugel gewählt wird, gegeben durch:
$$\rho = w \cdot \pi \cdot (1-\pi)^{w-1}$$
Unter Verwendung der Grenzen für $\pi$ erhalten wir für $w > 1$: $\rho > (1/2)(1-1/w)^{w-1} \ge 1/(2e)$. Für $w=1$ gilt $\rho \ge 1/(2e)$ ebenfalls.

\subsubsection{Wiederholung des Experiments}
Ein einzelnes Experiment, das mit Wahrscheinlichkeit $\ge 1/(2e)$ einen Zeugen liefert, ist möglicherweise nicht ausreichend. Durch mehrfache Wiederholung des "Experiments" kann die Wahrscheinlichkeit, genau einen Zeugen zu erhalten, erhöht werden. Bei $N$ Versuchen ist die Wahrscheinlichkeit des Scheiterns in allen Versuchen kleiner als $(1-1/(2e))^N \le e^{-N/(2e)}$. Wählt man $N = 2ec \log n$, so ist die Fehlerwahrscheinlichkeit kleiner als $1/n^c$. Dies bedeutet, dass nur für wenige Einträge $(i, j)$ ein Zeuge nicht gefunden wird.

Da $w_{ij}$ für jedes $(i, j)$ unterschiedlich sein kann und uns nicht bekannt ist, versucht der Algorithmus verschiedene Wahrscheinlichkeiten $\pi_s = 1/2^s$ für $s = 0, \ldots, \lceil \log n \rceil$. Dies wird erreicht, indem man ein Startset $R = \{1, \ldots, n\}$ hat und iterativ ein (1/2)-Sample aus $R$ zieht, wobei jedes Element unabhängig mit Wahrscheinlichkeit 1/2 ausgewählt wird. Im $i$-ten Iterationsschritt ist $k$ dann mit Wahrscheinlichkeit $1/2^i$ in $R$.

\subsubsection{Algorithmus BMM-WITNESS}
Der Algorithmus \texttt{BMM-WITNESS} findet Zeugen für die Boolesche Matrixmultiplikation:

\begin{algorithm}
	\caption{BMM-WITNESS(A, B)}
	\begin{algorithmic}[1]
		\State $W \gets \text{AB}$ \Comment{Initialisiere W (mit negativen Werten für fehlende Zeugen)}
		\For{$t \gets 1 \to 2ec \log n$}
		\State $R \gets \{1, \ldots, n\}$
		\For{$s \gets 0 \to \lceil \log n \rceil$}
		\State Compute $A^R$: $a_{ik}^R \gets [k \in R] \cdot k \cdot a_{ik}$ \Comment{$[k \in R]$ ist ein Indikator, 1 falls $k \in R$}
		\State $Z \gets A^R B$
		\For{all $i, j$}
		\If{$W_{ij} < 0$ AND $Z_{ij}$ is a witness} \Comment{Überprüfe, ob $a_{i,Z_{ij}}=1$ und $b_{Z_{ij},j}=1$}
		\State $W_{ij} \gets Z_{ij}$
		\EndIf
		\EndFor
		\State $R \gets \text{(1/2)-sample from R}$ \Comment{Jedes Element in R wird mit Wahrscheinlichkeit 1/2 ausgewählt}
		\EndFor
		\EndFor
		\For{all $i, j$}
		\If{$W_{ij} < 0$}
		\State Finde Zeugen für $i, j$ mittels Brute Force \Comment{Überprüfe jedes $k$, Zeit $O(n)$ pro Paar}
		\EndIf
		\EndFor
		\Return $W$
	\end{algorithmic}
\end{algorithm}

\subsubsection{Laufzeitanalyse}
Die Schleife in den Zeilen 4-10 gewährleistet, dass für jedes Paar $(i, j)$ eine Wahrscheinlichkeit nahe $1/w_{ij}$ ausprobiert wird. Dies geschieht $2ec \log n$ Mal, sodass die Wahrscheinlichkeit, einen Zeugen in dieser Schleife nicht zu finden, höchstens $1/n^c$ beträgt. Da es $n^2$ Paare $(i, j)$ gibt, ist die erwartete Anzahl fehlgeschlagener Paare höchstens $1/n^{c-2}$. Setzt man $c=1$, ist dies höchstens $n$, und die erwartete Zeit der Schleife in Zeile 11-13 (Brute Force) beträgt höchstens $O(n^2)$. Die dominierende Operation in der Schleife 4-10 ist die Matrixmultiplikation in Zeile 6. Daher beträgt die erwartete Gesamtlaufzeit $O(M(n) \log^2 n)$. Die Korrektheit des Algorithmus wird dadurch sichergestellt, dass Zeugen nur dann gesetzt werden, wenn sie direkt überprüft wurden.

\subsection{Nachfolger für kürzeste Pfade}

\subsubsection{Anwendung von Boolescher Matrixmultiplikation}
Wir nutzen nun die Techniken der Zeugensuche für die Boolesche Matrixmultiplikation, um eine Nachfolgermarix $S$ für das All-Paar-kürzeste-Pfade-Problem (APSP) zu erhalten. Für ein Paar von Knoten $(i, j)$ ist ein Knoten $s$ ein Nachfolger von $i$ auf einem kürzesten Pfad von $i$ nach $j$ genau dann, wenn die Distanz von $s$ nach $j$ genau eins kleiner ist als die Distanz von $i$ nach $j$, also $d_{sj} = d_{ij}-1$.

\subsubsection{Herausforderung bei der Definition von F}
Intuitiv könnte man eine Boolesche Matrix $F$ definieren, wobei $f_{sj}=1$ ist, wenn $d_{sj}=d_{ij}-1$. Dann könnte ein Nachfolger für $(i, j)$ als der $(i, j)$-Eintrag einer Zeugenmatrix für das Boolesche Produkt $AF$ gefunden werden. Das Problem hierbei ist jedoch, dass die Definition von $d_{sj}$ von $i$ abhängt, was bedeuten würde, dass wir alle $n$ Möglichkeiten durchprobieren müssten, was ineffizient wäre.

\subsubsection{Lösung mittels Modulo-3-Arithmetik}
Glücklicherweise kann man das Problem vereinfachen, indem man nur die Distanzen modulo 3 unterscheidet. Genauer gesagt, es ist ausreichend, drei verschiedene Matrizen $F^{(c)}$ für $c=0, 1, 2$ zu definieren:
$$f_{sj}^{(c)}=1 \quad \text{falls} \quad d_{sj} \equiv (c-1) \pmod 3$$
Obwohl diese Definition immer noch von $i$ abhängt, gibt es nur drei verschiedene Fälle, was die Berechnung vereinfacht.

\subsubsection{Algorithmus MM-APSP}
Der Algorithmus \texttt{MM-APSP} zur Berechnung der Nachfolgermarix für kürzeste Pfade ist wie folgt strukturiert:

\begin{algorithm}
	\caption{MM-APSP(A)}
	\begin{algorithmic}[1]
		\State $D \gets \text{MM-APSD}(A)$ \Comment{Berechne alle kürzesten Distanzen}
		\For{$c \gets 0, 1, 2$}
		\State Definiere $F^{(c)}$: $f_{sj}^{(c)}=1$ falls $d_{sj} \equiv (c-1) \pmod 3$
		\State $W^{(c)} \gets \text{BMM-WITNESS}(A, F^{(c)})$ \Comment{Verwende den Algorithmus aus VI.4}
		\EndFor
		\State Initialisiere $S$ als $n \times n$ Matrix
		\For{all $i, j$}
		\State $s_{ij} \gets w_{ij}^{(d_{ij} \pmod 3)}$ \Comment{Wähle den Zeugen aus der passenden $W^{(c)}$ Matrix}
		\EndFor
		\Return $S$
	\end{algorithmic}
\end{algorithm}

\subsubsection{Laufzeitanalyse}
Der Algorithmus \texttt{MM-APSD(A)} (All-Paar-kürzeste-Distanzen für Einheitsgewichte) hat eine Laufzeit von $O(M(n) \log n)$. Der Hauptteil des \texttt{MM-APSP}-Algorithmus beinhaltet drei Aufrufe von \texttt{BMM-WITNESS}, einmal für jedes $c \in \{0, 1, 2\}$. Jeder Aufruf von \texttt{BMM-WITNESS} hat eine erwartete Laufzeit von $O(M(n) \log^2 n)$. Daher ist die erwartete Gesamtlaufzeit des \texttt{MM-APSP}-Algorithmus $O(M(n) \log^2 n)$.
\end{document}